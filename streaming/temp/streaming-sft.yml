# Example configuration for streaming SFT training
# This enables training on datasets larger than memory by streaming them from HuggingFace Hub

base_model: HuggingFaceTB/SmolLM2-135M

# Enable streaming mode for datasets
streaming: true

# When using streaming, max_steps is required
max_steps: 10000

# Training datasets - these will be streamed
datasets:
  - path: tatsu-lab/alpaca
    type: alpaca
    split: train

# pretraining_dataset:
#   - path: tatsu-lab/alpaca
#     type: alpaca
#     split: train

# Dataset configuration
sequence_len: 1024
sample_packing: true
pretrain_multipack_attn: true
pretrain_multipack_buffer_size: 10000
special_tokens:
  pad_token: <|endoftext|>

# Training hyperparameters
gradient_accumulation_steps: 1
micro_batch_size: 1
optimizer: adamw_torch
lr_scheduler: cosine
learning_rate: 2e-5

# Enable efficient training
bf16: auto
tf32: false
gradient_checkpointing: true
flash_attention: true  # Enable flash attention with multipack

# Logging and checkpointing
logging_steps: 10
output_dir: ./outputs/streaming-model

# Wandb
wandb_project: streaming-v3
wandb_entity: axolotl-ai
wandb_watch: all
wandb_name:
wandb_log_model: end

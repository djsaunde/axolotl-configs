# Example configuration for streaming SFT training
# This enables training on datasets larger than memory by streaming them from HuggingFace Hub

base_model: HuggingFaceTB/SmolLM2-135M

# When using streaming, max_steps is required
max_steps: 10000

# # Training datasets - these will be streamed
# datasets:
#   - path: tatsu-lab/alpaca
#     type: alpaca
#     split: train

pretraining_dataset:
  - path: tatsu-lab/alpaca
    type: alpaca
    # type: pretrain
    split: train

# Dataset configuration
sequence_len: 2048
sample_packing: true
pretrain_multipack_attn: true
pretrain_multipack_buffer_size: 10000
special_tokens:
  pad_token: <|endoftext|>

# Training hyperparameters
gradient_accumulation_steps: 4
micro_batch_size: 1
optimizer: adamw_torch
lr_scheduler: cosine
learning_rate: 2e-5

# Enable efficient training
bf16: auto
tf32: false
gradient_checkpointing: true
flash_attention: true  # Enable flash attention with multipack

# Logging and checkpointing
logging_steps: 10
eval_steps: 100
save_steps: 200
output_dir: ./outputs/streaming-model

# Wandb
# wandb_project: streaming-v2
# wandb_entity: axolotl-ai
# wandb_watch: all
# wandb_name:
# wandb_log_model: end
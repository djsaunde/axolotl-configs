base_model: HuggingFaceTB/SmolLM2-135M
datasets:
  - path: mhenrichsen/alpaca_2k_test
    type: alpaca
  - path: tatsu-lab/alpaca
    type: alpaca

# Streaming config
streaming: true
max_steps: 3

# Round robin mixing strategy
dataset_mixing_strategy: round_robin

# Basic training config
micro_batch_size: 1
gradient_accumulation_steps: 1
val_set_size: 0.0
learning_rate: 1e-5
optimizer: adamw_torch_fused
lr_scheduler: cosine
save_safetensors: true
bf16: auto
use_tensorboard: true
save_first_step: false

# Model config
flash_attention: true
sequence_len: 1024
sample_packing: false
special_tokens:
  pad_token: <|endoftext|>
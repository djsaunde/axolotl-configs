base_model: axolotl-ai-co/mixtral-1b-moe-test
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer

# Debug config to test numerical precision
# Uses float32 instead of bfloat16

plugins:
  - axolotl.integrations.moe_kernels.plugin.MoeOptimizedPlugin

moe_kernels: true
moe_group_size: 128
moe_persistent_kernel: true

trust_remote_code: true

load_in_8bit: false
load_in_4bit: false

# Small dataset for debugging
datasets:
  - path: mlabonne/FineTome-100k
    type: chat_template
    field_messages: conversations
    message_field_role: from
    message_field_content: value
    split: train[:1%]  # Very small for debugging

dataset_prepared_path: last_run_prepared
output_dir: ./outputs/mixtral-1b-debug

model_config:
  output_router_logits: true

sequence_len: 512  # Small for debugging
sample_packing: false  # Disable packing for simpler debugging
pad_to_sequence_len: true

# Minimal training for debugging
num_epochs: 1
gradient_accumulation_steps: 1
micro_batch_size: 1
max_steps: 10  # Just a few steps

optimizer: adamw_torch
lr_scheduler: cosine
learning_rate: 2e-5

# Use float32 for debugging precision issues
bf16: false
fp16: false
tf32: false

gradient_checkpointing: false  # Simpler for debugging
flash_attention: false  # Simpler for debugging

logging_steps: 1
save_steps: 1000

special_tokens:
  pad_token: "<pad>"
  eos_token: "</s>"
  bos_token: "<s>"
  unk_token: "<unk>"

chat_template: chatml

strict: false
debug: true

wandb_project: moe-debug
wandb_entity: axolotl-ai
wandb_name: mixtral-1b-debug-float32
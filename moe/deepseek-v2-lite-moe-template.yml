base_model: deepseek-ai/DeepSeek-V2-Lite-Chat
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer

# Note: MoE kernels not yet supported for DeepSeek-V2 architecture
# The following would need to be implemented in the plugin first:
# plugins:
#   - axolotl.integrations.moe_kernels.plugin.MoeOptimizedPlugin
# 
# moe_kernels: true
# moe_group_size: 64  # Potentially smaller for DeepSeek-V2
# moe_persistent_kernel: true

trust_remote_code: true

load_in_8bit: false
load_in_4bit: false

# Dataset Configuration
datasets:
  - path: mlabonne/FineTome-100k
    type: chat_template
    field_messages: conversations
    message_field_role: from
    message_field_content: value
    split: train[:20%]

dataset_prepared_path: last_run_prepared
output_dir: ./outputs/deepseek-v2-lite-moe-template

# Model Configuration
model_config:
  output_router_logits: true  # Enable for load balancing loss

# Sequence Configuration
sequence_len: 4096  # DeepSeek V2 context length
sample_packing: true
pad_to_sequence_len: true

# Training Configuration
num_epochs: 1
gradient_accumulation_steps: 2
micro_batch_size: 1

# Optimizer and Learning Rate
optimizer: adamw_torch
lr_scheduler: cosine
learning_rate: 1e-5  # Lower LR for DeepSeek models
warmup_ratio: 0.05
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1e-8

# Mixed Precision and Performance
bf16: true
fp16: false
tf32: true
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
flash_attention: true

# FSDP Configuration for DeepSeek V2 (large model)
fsdp:
  - full_shard
  - auto_wrap
fsdp_config:
  fsdp_limit_all_gathers: true
  fsdp_sync_module_states: true
  fsdp_offload_params: true  # Enable CPU offloading
  fsdp_use_orig_params: false
  fsdp_cpu_ram_efficient_loading: true
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_transformer_layer_cls_to_wrap: DeepseekV2MoE
  fsdp_state_dict_type: SHARDED_STATE_DICT
  fsdp_sharding_strategy: FULL_SHARD

# Logging and Saving
logging_steps: 10

# Special tokens
special_tokens:
  bos_token: "<｜begin▁of▁sentence｜>"
  eos_token: "<｜end▁of▁sentence｜>"
  pad_token: "<｜pad｜>"

# Chat template
chat_template: deepseek_v2

# Additional Configuration
max_grad_norm: 1.0
strict: false

# Debugging options
debug: true

# Wandb Configuration
wandb_project: moe-kernels
wandb_entity: axolotl-ai
wandb_watch:
wandb_name: deepseek-v2-lite-template-4096sl-pack
wandb_log_model:
# Consistent MoE Config Naming

All configs now have **consistent naming** for easy comparison and use.

## **ğŸ“ Config Structure:**

```
/workspace/configs/moe/
â”œâ”€â”€ mixtral-1b-baseline.yml          # Mixtral 1B WITHOUT MoE kernels  
â”œâ”€â”€ mixtral-1b-optimized.yml         # Mixtral 1B WITH MoE kernels
â”œâ”€â”€ mixtral-1b-test.yml              # Mixtral 1B fast test (WITH kernels)
â”œâ”€â”€ 
â”œâ”€â”€ qwen2-moe-1b-baseline.yml        # Qwen2-MoE 1B WITHOUT MoE kernels
â”œâ”€â”€ qwen2-moe-1b-optimized.yml       # Qwen2-MoE 1B WITH MoE kernels  
â”œâ”€â”€ qwen2-moe-1b-test.yml            # Qwen2-MoE 1B fast test (WITH kernels)
â”œâ”€â”€
â”œâ”€â”€ deepseek-v3-1b-baseline.yml      # DeepSeek-V3 1B WITHOUT MoE kernels
â”œâ”€â”€ deepseek-v3-1b-optimized.yml     # DeepSeek-V3 1B WITH MoE kernels
â””â”€â”€ deepseek-v3-1b-test.yml          # DeepSeek-V3 1B fast test (WITH kernels)
```

## **ğŸ”„ Config Comparison Pairs:**

### **Mixtral 1B:**
| **Baseline (No Kernels)** | **Optimized (With Kernels)** | **Fast Test (With Kernels)** |
|---------------------------|-------------------------------|------------------------------|
| `mixtral-1b-baseline.yml` | `mixtral-1b-optimized.yml` | `mixtral-1b-test.yml` |

### **Qwen2-MoE 1B:**
| **Baseline (No Kernels)** | **Optimized (With Kernels)** | **Fast Test (With Kernels)** |
|---------------------------|-------------------------------|------------------------------|
| `qwen2-moe-1b-baseline.yml` | `qwen2-moe-1b-optimized.yml` | `qwen2-moe-1b-test.yml` |

### **DeepSeek-V3 1B:**
| **Baseline (No Kernels)** | **Optimized (With Kernels)** | **Fast Test (With Kernels)** |
|---------------------------|-------------------------------|------------------------------|
| `deepseek-v3-1b-baseline.yml` | `deepseek-v3-1b-optimized.yml` | `deepseek-v3-1b-test.yml` |

---

## **âš¡ Quick Commands:**

### **Performance Comparison (A/B Test):**
```bash
# Test standard PyTorch MoE (baseline)
axolotl train /workspace/configs/moe/deepseek-v3-1b-baseline.yml

# Test optimized MoE kernels  
axolotl train /workspace/configs/moe/deepseek-v3-1b-optimized.yml
```

### **Fast Testing (50-100 steps):**
```bash
axolotl train /workspace/configs/moe/deepseek-v3-1b-test.yml
axolotl train /workspace/configs/moe/mixtral-1b-test.yml
axolotl train /workspace/configs/moe/qwen2-moe-1b-test.yml
```

### **Full Training:**
```bash
axolotl train /workspace/configs/moe/deepseek-v3-1b-optimized.yml
axolotl train /workspace/configs/moe/mixtral-1b-optimized.yml
axolotl train /workspace/configs/moe/qwen2-moe-1b-optimized.yml
```

---

## **ğŸ“Š Config Types Explained:**

### **ğŸ”¸ Baseline Configs:**
- âŒ **No MoE kernel optimizations**
- ğŸ“ˆ Standard PyTorch MoE implementation  
- ğŸ¯ Used for performance comparison
- ğŸ“Š WandB project: `moe-baseline`

### **ğŸ”¸ Optimized Configs:**
- âœ… **MoE kernel optimizations enabled**
- âš¡ Contiguous grouped GEMM operations
- ğŸš€ Token sorting and memory coalescence
- ğŸ“Š WandB project: `moe-kernels`

### **ğŸ”¸ Test Configs:**
- âœ… **MoE kernel optimizations enabled**
- âš¡ Small datasets (5% data)
- ğŸƒ Fast iteration (50-100 max steps)
- ğŸ“Š WandB project: `moe-kernels-test`

---

## **ğŸ¯ Recommended Testing Workflow:**

1. **Start with fast test**: `deepseek-v3-1b-test.yml` (50 steps)
2. **Run baseline comparison**: `deepseek-v3-1b-baseline.yml` vs `deepseek-v3-1b-optimized.yml`
3. **Scale to other architectures**: Mixtral and Qwen2-MoE tests
4. **Full training**: Once satisfied with optimizations

All configs use your **private 1B models** (`axolotl-ai-co/*-test`) for consistent, fast testing!
# base_model: HuggingFaceTB/SmolLM2-135M
# base_model: HuggingFaceTB/SmolLM2-1.7B
# base_model: meta-llama/Llama-3.2-3B
base_model: meta-llama/Llama-3.1-8B
datasets:
  - path: teknium/GPT4-LLM-Cleaned
    type: alpaca

gradient_accumulation_steps: 1
num_epochs: 1
learning_rate: 1e-4
micro_batch_size: 1
sequence_len: 8192
special_tokens:
  pad_token: <|endoftext|>
max_steps: 100

sdp_attention: true
pad_to_seq_len: true
# flash_attention: true
# flex_attention: true
sample_packing: true

dataset_prepared_path: ./last_run_prepared
output_dir: /workspace/data/model-out

# bf16: true
# fp8: true
# fp8_enable_fsdp_float8_all_gather: true

# gradient_checkpointing: true

# torch_compile: true

fsdp:
  - full_shard
  - auto_wrap

fsdp_version: 2
fsdp_config:
  offload_params: false
  # cpu_ram_efficient_loading: true
  cpu_ram_efficient_loading: false
  auto_wrap_policy: TRANSFORMER_BASED_WRAP
  transformer_layer_cls_to_wrap: LlamaDecoderLayer
  state_dict_type: FULL_STATE_DICT
  sharding_strategy: FULL_SHARD
  reshard_after_forward: true
  # activation_checkpointing: false
  activation_checkpointing: true

# wandb_project: fp8-v2
# wandb_entity: axolotl-ai
# wandb_watch: 
# # wandb_name: llama3.1-8b-bf16-fsdp2-8192seqlen-ac-pack-pad2sl
# wandb_name: llama3.1-8b-fp8-allgather-fsdp2-8192seqlen-ac-pack-pad2sl
# wandb_log_model: 

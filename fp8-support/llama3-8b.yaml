# base_model: HuggingFaceTB/SmolLM2-135M
# base_model: HuggingFaceTB/SmolLM2-1.7B
base_model: meta-llama/Llama-3.1-8B
datasets:
  - path: mhenrichsen/alpaca_2k_test
    type: alpaca

gradient_accumulation_steps: 1
learning_rate: 1e-4
val_set_size: 0.1
micro_batch_size: 2
sequence_len: 2048
special_tokens:
  pad_token: <|endoftext|>

# sdp_attention: true
flash_attention: true
sample_packing: true

dataset_prepared_path: ./last_run_prepared
output_dir: /workspace/data/model-out

accelerator_config:
  mixed_precision: fp8  
  fp8_config:
    backend: AO
    # recipe_name: tensorwise
    config:
      enable_fsdp_float8_all_gather: true
      force_recompute_fp8_weight_in_bwd: true

# gradient_checkpointing: true

torch_compile: true
# torch_compile_backend: 
# torch_compile_mode: max-autotune

# wandb_project: fp8
# wandb_entity: axolotl-ai
# wandb_watch: all
# wandb_name: smollm2-1.7b-fp8-compile
# wandb_log_model: 

fsdp:
  - full_shard
  - auto_wrap

fsdp_config:
  fsdp_version: 2
  fsdp_offload_params: false
  # fsdp_cpu_ram_efficient_loading: true
  fsdp_cpu_ram_efficient_loading: false
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer
  fsdp_state_dict_type: FULL_STATE_DICT
  fsdp_sharding_strategy: FULL_SHARD
  fsdp_reshard_after_forward: true
  fsdp_activation_checkpointing: true
base_model: unsloth/gemma-3-270m

gradient_accumulation_steps: 4
micro_batch_size: 1
num_epochs: 1
optimizer: adamw_bnb_8bit
lr_scheduler: cosine
learning_rate: 0.00001

load_in_4bit: true
adapter: qlora

sequence_len: 4096

lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_linear: true

bf16: true
tf32: false

logging_steps: 1
eager_attention: true

loss_watchdog_threshold: 5.0
loss_watchdog_patience: 3

rl: dpo
datasets:
    - path: nbeerbower/gutenberg-moderne-dpo
      split: train
      type: chatml.prompt_pairs
dataset_prepared_path: last_run_prepared
# val_set_size: 0.08
output_dir: ./outputs/lora-out

profiler_steps: 3
max_steps: 3

dpo_disable_output_fp32: true

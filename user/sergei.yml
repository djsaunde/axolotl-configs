# axolotl train axolotl-qwen3-all.yaml --deepspeed ../deepspeed_configs/zero3_bf16_cpuoffload_params.json

base_model: Qwen/Qwen3-1.7B
trust_remote_code: true

# output_dir: ./qwen3-axolotl-all-v9
# wandb_name: qwen3-axolotl-all-v9

plugins:
  - axolotl.integrations.liger.LigerPlugin
liger_rope: true
liger_rms_norm: true
liger_glu_activation: true
liger_layer_norm: true
liger_fused_linear_cross_entropy: true

chat_template: qwen3

datasets:
  - path: mhenrichsen/alpaca_2k_test
    type: alpaca

# datasets:
#   - path: ./train.jsonl
#     ds_type: json
#     type: chat_template
#     split: train
#     field_messages: messages
#     message_field_role: role
#     message_field_content: content
# test_datasets:
#   - path: ./validation.jsonl
#     ds_type: json
#     type: chat_template
#     split: train
#     field_messages: messages
#     message_field_role: role
#     message_field_content: content

sequence_len: 29453
train_on_inputs: false
sample_packing: false
eval_sample_packing: false
pad_to_sequence_len: false

learning_rate: 3e-5
num_epochs: 3
max_grad_norm: 1.0
gradient_accumulation_steps: 4 # This is per GPU
micro_batch_size: 1
optimizer: adamw_torch_fused
lr_scheduler: cosine
weight_decay: 0.01

bf16: true
fp16: false
tf32: false
load_in_8bit: false

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
logging_steps: 1
flash_attention: true
eager_attention:

warmup_ratio: 0.05
evals_per_epoch: 2
saves_per_epoch: 1
save_total_limit: 8

deepspeed: deepspeed_configs/zero3_bf16_cpuoffload_params.json

# save_only_model: false
# resume_from_checkpoint: path/to/checkpoint

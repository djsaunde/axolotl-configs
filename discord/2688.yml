#base_model: NousResearch/Meta-Llama-3.1-8B
#base_model: NousResearch/Hermes-3-Llama-3.1-8B
#base_model: meta-llama/Llama-3.1-8B-Instruct
#base_model: meta-llama/Llama-3.2-11B-Vision-Instruct
base_model: unsloth/Phi-4
seed: 1337
#tokenizer_num_proc: 189
#dataset_processes: 189
plugins:
  - axolotl.integrations.liger.LigerPlugin
liger_rope: true
liger_rms_norm: true
liger_glu_activation: true
liger_fused_linear_cross_entropy: true

dataset_prepared_path: /workspace/axolotl/last_run_prepared
strict: false

# Save model as safetensors (require safetensors package)
save_safetensors: true
auto_resume_from_checkpoints: true
# Custom JSON dataset configuration
datasets:
  - path: /workspace/data/datasets/subset_1m_dataset/data-00000-of-00001.arrow
    type: completion
    # data_files:
    #   - /workspace/data/datasets/subset_1m_dataset/data-00000-of-00001.arrow
    # ds_type: json
    split: train
    field: text

val_set_size: 0

# Sequence and batch configuration
sequence_len: 128  # Adjust based on your model's needs
sample_packing: false
pad_to_sequence_len: true

# Optional W&B logging setup
#wandb_project: your_project_name  # Set your Weights & Biases project name
#wandb_entity: your_entity_name
#wandb_watch: false
#wandb_name: your_run_name
#wandb_log_model: true  # Set to true if you want to log the model in W&B

# Training parameters
gradient_accumulation_steps: 1  # From your Python script
micro_batch_size: 250  # Batch size from your script
num_epochs: 1  # Number of epochs from your script
#optimizer: adamw_torch
optimizer: adopt_adamw
adam_beta1: 0.90
adam_beta2: 0.999
#optimizer_kwargs:
#  betas: [0.85, 0.999]
lr_scheduler: cosine
learning_rate: 1e-6  # Learning rate from your Python code
max_grad_norm: 0.15
# Mixed precision training
train_on_inputs: false
group_by_length: false
bf16: auto
fp16:  # Enable FP16 based on your system's capabilities
tf32: false
#gpu_memory_limit: 20GiB

# Gradient checkpointing and memory optimization
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

# Logging and evaluation settings
logging_steps: 1
xformers_attention: 
flash_attention: true

# Save and evaluation settings
warmup_steps: 25
#evals_per_epoch: 1
save_strategy: steps
save_steps: 200  # Save every 1500 steps, matching your script
saves_per_epoch:  # Leave this empty since it's mutually exclusive with save_steps
save_total_limit: 2  # Save a maximum of 5 checkpoints
debug: false
deepspeed: /workspace/axolotl/deepspeed_configs/zero3_bf16.json
weight_decay: 0.01

# Special tokens configuration
special_tokens:
  pad_token: <|dummy_87|>
  eos_token: <|im_end|>

# Add extra tokens.
tokens:

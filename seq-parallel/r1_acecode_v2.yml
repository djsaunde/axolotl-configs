base_model: Qwen/Qwen2.5-3B-Instruct
# Automatically upload checkpoint and final model to HF
# hub_model_id: username/custom_model_name

load_in_8bit: false
load_in_4bit: false
strict: false

# torch_compile: true

vllm:
    host: 0.0.0.0
    port: 8000
    tensor_parallel_size: 2
    gpu_memory_utilization: 0.85
    dtype: auto

rl: grpo
trl:
  beta: 0.001    
  use_vllm: true
  vllm_server_host: 0.0.0.0
  vllm_server_port: 8000
  vllm_server_timeout: 300
  reward_funcs:
    - grpo_code.soft_format_reward_func
    - grpo_code.code_execution_reward_func
    - grpo_code.answer_execution_reward_func

  num_generations: 2
  max_completion_length: 512
  log_completions: false

chat_template: qwen_25
datasets:
  - path: axolotl-ai-co/AceCode-87K
    type: grpo_code.axolotl_acecode_transform
    split: train
    
dataset_prepared_path: /workspace/data/last_run_prepared
dataset_processes:
skip_prepare_dataset: true
val_set_size: 0.0

dataloader_prefetch_factor: 32
dataloader_num_workers: 2
dataloader_pin_memory: true

gc_steps: 1
sequence_len: 1024
sample_packing: false
eval_sample_packing: false
pad_to_sequence_len: false

gradient_accumulation_steps: 1
micro_batch_size: 2 # 32
num_epochs: 1
max_steps: 2500

optimizer: adamw_torch_fused
lr_scheduler: warmup_stable_decay
lr_scheduler_kwargs:
  num_stable_steps: 1500
  num_decay_steps: 500
  min_lr_ratio: 0.1
  num_cycles: 0.5
  
learning_rate: 5.3e-6
max_grad_norm: 1.0

train_on_inputs: false
group_by_length: false

bf16: true
tf32: true  
early_stopping_patience:
resume_from_checkpoint:
local_rank:
logging_steps: 1
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
flash_attention: true

warmup_steps: 500
evals_per_epoch: 0
saves_per_epoch: 0
save_steps: 0.5

# wandb_project: seq-parallel-grpo-v4
# wandb_entity: axolotl-ai
# wandb_watch: all
# wandb_name: sp2
# wandb_log_model: 

seed: 0

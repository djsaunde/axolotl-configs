# Config for testing ring-flash-attn sequence parallelism
# Optimized for LLaMA 3.1 8B model

# base_model: meta-llama/Meta-Llama-3.1-8B
base_model: meta-llama/Llama-3.2-1B
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
is_llama_derived_model: true

datasets:
  - path: teknium/GPT4-LLM-Cleaned
    type: alpaca
micro_batch_size: 1
gradient_accumulation_steps: 1
learning_rate: 1e-4

gradient_accumulation_steps: 1
micro_batch_size: 1
num_epochs: 1
optimizer: adamw_torch_fused
lr_scheduler: cosine
learning_rate: 0.0002

train_on_inputs: false
group_by_length: false
bf16: auto
fp16:
tf32: false

sample_packing: false
pad_to_sequence_len: true

special_tokens:
  pad_token: "<|end_of_text|>"

save_strategy: "no"
eval_strategy: "no"

load_in_8bit: false
load_in_4bit: false
device_map: auto 

flash_attention: true
torch_dtype: bfloat16

gradient_checkpointing: false
gradient_checkpointing_kwargs:
  use_reentrant: false

deepspeed: deepspeed_configs/zero2.json
